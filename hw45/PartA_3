import numpy as np
import matplotlib.pyplot as plt

# Given data
X = np.array([-1, -0.9, -0.8, -0.7, -0.6, -0.5, -0.4, -0.3, -0.2, -0.1, 0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])
Y = np.array([-0.96, -0.577, -0.073, 0.377, 0.641, 0.66, 0.461, 0.134, -0.201, -0.434, -0.5, -0.393, -0.165, 0.099, 0.307, 0.396, 0.345, 0.182, -0.031, -0.219, -0.321])

# Define the functional approximation function
def f(X, x0, x1, x2, x3, x4):
    return x0 + x1 * np.sin(x2 * X) + x3 * np.cos(x4 * X)

# Define the objective function
def objective_function(X, Y, x0, x1, x2, x3, x4):
    return np.sum((Y - f(X, x0, x1, x2, x3, x4))**2)

# Adjusted gradient descent function with reduced learning rate and increased tolerance
def gradient_descent(X, Y, learning_rate=0.001, max_iter=1000, tol=1e-4):
    # Adjusted initial guess for parameters
    x0, x1, x2, x3, x4 =  7, 9, 20, 34, 6
    cost_history = []
    for i in range(max_iter):
        # Compute gradients
        grad_x0 = -2 * np.sum(Y - f(X, x0, x1, x2, x3, x4))
        grad_x1 = -2 * np.sum((Y - f(X, x0, x1, x2, x3, x4)) * np.sin(x2 * X))
        grad_x2 = -2 * np.sum((Y - f(X, x0, x1, x2, x3, x4)) * x1 * np.cos(x2 * X) * X)
        grad_x3 = -2 * np.sum((Y - f(X, x0, x1, x2, x3, x4)) * np.cos(x4 * X))
        grad_x4 = 2 * np.sum((Y - f(X, x0, x1, x2, x3, x4)) * x3 * np.sin(x4 * X) * X)
        # Update parameters
        x0 -= learning_rate * grad_x0
        x1 -= learning_rate * grad_x1
        x2 -= learning_rate * grad_x2
        x3 -= learning_rate * grad_x3
        x4 -= learning_rate * grad_x4
        # Compute and store cost
        cost = objective_function(X, Y, x0, x1, x2, x3, x4)
        cost_history.append(cost)
        # Check convergence
        if cost < tol:
            break
    return x0, x1, x2, x3, x4, cost_history

# Run adjusted gradient descent
x0, x1, x2, x3, x4, cost_history = gradient_descent(X, Y)

# Print the adjusted optimized coefficients
print("Adjusted optimized coefficients:")
print("x0 =", x0)
print("x1 =", x1)
print("x2 =", x2)
print("x3 =", x3)
print("x4 =", x4)

# Plot the adjusted fitted function along with the given data
plt.scatter(X, Y, label='Data')
X_range = np.linspace(-1, 1, 100)
plt.plot(X_range, f(X_range, x0, x1, x2, x3, x4), color='red', label='Fitted function')
plt.xlabel('X')
plt.ylabel('Y')
plt.title('Adjusted Fitted Function vs. Given Data')
plt.legend()
plt.show()

# Plot cost vs iteration for adjusted gradient descent
plt.plot(cost_history, marker='o')
plt.xlabel('Iteration')
plt.ylabel('Cost')
plt.title('Adjusted Cost vs Iteration')
plt.grid(True)
plt.show()
